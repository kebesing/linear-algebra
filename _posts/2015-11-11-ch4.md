---
layout: post
title: Chapter 4 Wrap
---

Today we cleaned up most of the outstanding items from Chapter Four.

We discussed how the process we use for Least Squares models a function by looking
only at its values. If we only care about those values at a few points, it becomes
possible to take the values at just those points and make a vector out of it.
Hence the functions become vectors, and we are just looking at orthogonal
projection as the tool to solve our problems.

We also discussed how to count dimensions in this case, and why it should not be
surprising that we can fit a degree \\(n-1\\) polynomial to a "general" cloud of
\\(n\\) points.

  * \#136 Giarusso
  * \#137 Schultz
  * \#144 Stanley

and I discussed \#141 a bit. There is a funny connection between finding the QR
decomposition for a matrix \\(A\\) and doing Gauss-Jordan Elimination on
\\(A^TA\\). I had trouble with my tech during class. It turns out that SMC crashed
right about the time I tried to log in. I found out later. Anyway, I'll clean up
the worksheet I had and share it with you all by email later.

#### For Friday, 13 November

Prepare for the assessments on these two standards:

  * Approximate Solutions and Least Squares
  * (advanced) The four subspaces and matrices as transformations

note that I renamed that second one. I think this is a better name.

We will save the assessment on Gram-Schmidt and related issues until next week
after we have studied determinants.

#### By the way,

Properties of the determinant function is our next topic. It is not too bad, and
we get to have a big theorem, too.
